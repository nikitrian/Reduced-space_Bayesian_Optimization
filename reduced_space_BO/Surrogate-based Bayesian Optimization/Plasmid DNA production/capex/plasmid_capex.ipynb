{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import utils\n",
    "import tqdm\n",
    "from torch import nn\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(10)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "metric = 'capex'\n",
    "inputlength = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm_norm(arr, normP):\n",
    "    \"\"\"\n",
    "    Min max normalisation\n",
    "    \"\"\"\n",
    "    arrmax = normP[0] \n",
    "    arrmin = normP[1]\n",
    "    return (arr - arrmin)/(arrmax - arrmin)\n",
    "\n",
    "def mm_rev(norm, normP):\n",
    "    \"\"\"\n",
    "    Reverse min max normalisation\n",
    "    \"\"\"\n",
    "    arrmax = normP[0] \n",
    "    arrmin = normP[1]\n",
    "    return norm*(arrmax - arrmin) + arrmin\n",
    "\n",
    "with open('device.txt', 'w') as f:\n",
    "    f.write(device)\n",
    "    f.write(f'\\n{torch.cuda.is_available()}')\n",
    "\n",
    "\n",
    "def save_pkl(data, save_name):\n",
    "    \"\"\"\n",
    "    Saves .pkl file from of data in folder: tmp/\n",
    "    \"\"\"\n",
    "    with open(save_name, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    #print(f'File saved at: {save_name}')\n",
    "    return None\n",
    "\n",
    "def load_pkl(file_name):\n",
    "    \"\"\"\n",
    "    Loads .pkl file from path: file_name\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    print(f'Data loaded: {file_name}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReLUNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    ReLU neural network structure.\n",
    "    1 hidden layers, 1 output layer.\n",
    "    size of input, output, and hidden layers are specified\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden, n_output, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.hidden_layers.append(torch.nn.Linear(n_input, n_hidden))\n",
    "            else:\n",
    "                self.hidden_layers.append(torch.nn.Linear(n_hidden, n_hidden))\n",
    "\n",
    "\n",
    "        self.output = nn.Linear(n_hidden, n_output)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.relu(layer(x))\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(fn, train_set):\n",
    "    \"\"\"\n",
    "    Load ANN model for inference\n",
    "    \"\"\"\n",
    "\n",
    "    P = load_pkl(fn)\n",
    "    SD = P['state_dict']\n",
    "    \n",
    "    structure = P['structure']\n",
    "    \n",
    "    \n",
    "    inputs = train_set[0].shape[1]\n",
    "    outputs = train_set[1].shape[1]\n",
    "    m = ReLUNet(inputs, structure[2], outputs, structure[3]).to(torch.float64)\n",
    "    m.load_state_dict(SD)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "\n",
    "def normalize_x(params, space):\n",
    "    bounds = torch.tensor([var['domain'] for var in space]).to(params).t()\n",
    "    params = normalize(params, bounds)\n",
    "    return params\n",
    "\n",
    "def unnormalize_x(params, space):\n",
    "    bounds = torch.tensor([var['domain'] for var in space]).to(params).t()\n",
    "    params = unnormalize(params, bounds)\n",
    "    return params\n",
    "\n",
    "def wrap_X(X, space):\n",
    "\n",
    "    def _wrap_row(row):\n",
    "        wrapped_row = {}\n",
    "        for i, x in enumerate(row):\n",
    "            wrapped_row[space[i]['name']] = x.item()\n",
    "        \n",
    "            if space[i]['type'] == 'discrete':\n",
    "                wrapped_row[space[i]['name']] = int(np.round(x.item()))\n",
    "        return wrapped_row\n",
    "    \n",
    "    wrapped_X = []\n",
    "    for i in range(X.shape[0]):\n",
    "        wrapped_X.append(_wrap_row(X[i]))\n",
    "        \n",
    "    return wrapped_X\n",
    "\n",
    "\n",
    "def unwrap_X(parameters, space):\n",
    "\n",
    "    X = torch.zeros(len(parameters), len(space),\n",
    "                    dtype=torch.float64)\n",
    "    for i, p in enumerate(parameters):\n",
    "        x = [p[var['name']] for var in space]\n",
    "        X[i] = torch.tensor(x, dtype=torch.float64)\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "space = [\n",
    "    #{'name': 'flask_time', 'type': 'continuous', 'domain': (21600, 108000)},\n",
    "    #{'name': 'seed_time', 'type': 'continuous', 'domain': (43200, 108000)},\n",
    "    #{'name': 'seed_fed_batch', 'type': 'continuous', 'domain': (0.001636, 0.004908)},\n",
    "    {'name': 'seed_batch_med', 'type': 'continuous', 'domain': (0.010225325, 0.030675975)},\n",
    "    #{'name': 'seed_conversion', 'type': 'continuous', 'domain': (0.9, 0.98)},\n",
    "    #{'name': 'main_time', 'type': 'continuous', 'domain': (64800, 144000)},\n",
    "    {'name': 'fed_batch', 'type': 'continuous', 'domain': (0.0190875, 0.0572625)},\n",
    "    {'name': 'batch_med', 'type': 'continuous', 'domain': (0.102253255, 0.306759765)},\n",
    "    {'name': 'conversion', 'type': 'continuous', 'domain': (0.9, 0.98)},\n",
    "    #{'name': 'solid_conc', 'type': 'continuous', 'domain': (50, 300)},\n",
    "    {'name': 'res_vol', 'type': 'continuous', 'domain': (0.28903585,  0.86710755)},\n",
    "    #{'name': 'equi1', 'type': 'continuous', 'domain': (0.01, 0.05)},\n",
    "    {'name': 'dia1', 'type': 'discrete', 'domain': (10, 50)},\n",
    "    #{'name': 'flush1', 'type': 'continuous', 'domain': (0.0015,  0.0045)},\n",
    "    #{'name': 'equi2', 'type': 'continuous', 'domain': (0.01, 0.05)},\n",
    "    {'name': 'dia2', 'type': 'discrete', 'domain': (10, 50)},\n",
    "    {'name': 'flush2', 'type': 'continuous', 'domain': (0.00067,  0.002)},\n",
    "    #{'name': 'failure', 'type': 'continuous', 'domain': (0.0, 0.1)},\n",
    "]\n",
    "\n",
    "space2 = [\n",
    "    #{'name': 'flask_time', 'type': 'continuous', 'domain': (21600*0.9, 108000*1.1)},\n",
    "    #{'name': 'seed_time', 'type': 'continuous', 'domain': (43200*0.9, 108000*1.1)},\n",
    "    #{'name': 'seed_fed_batch', 'type': 'continuous', 'domain': (0.001636*0.9, 0.004908*1.1)},\n",
    "    {'name': 'seed_batch_med', 'type': 'continuous', 'domain': (0.010225325*0.9, 0.030675975*1.1)},\n",
    "    #{'name': 'seed_conversion', 'type': 'continuous', 'domain': (0.9*0.9, 0.98*1.1)},\n",
    "    #{'name': 'main_time', 'type': 'continuous', 'domain': (64800*0.9, 144000*1.1)},\n",
    "    {'name': 'fed_batch', 'type': 'continuous', 'domain': (0.0190875*0.9, 0.0572625*1.1)},\n",
    "    {'name': 'batch_med', 'type': 'continuous', 'domain': (0.102253255*0.9, 0.306759765*1.1)},\n",
    "    {'name': 'conversion', 'type': 'continuous', 'domain': (0.9*0.9, 0.98*1.1)},\n",
    "    #{'name': 'solid_conc', 'type': 'continuous', 'domain': (50*0.9, 300*1.1)},\n",
    "    {'name': 'res_vol', 'type': 'continuous', 'domain': (0.28903585*0.9,  0.86710755*1.1)},\n",
    "    #{'name': 'equi1', 'type': 'continuous', 'domain': (0.01*0.9, 0.05*1.1)},\n",
    "    {'name': 'dia1', 'type': 'discrete', 'domain': (10*0.9, 50*1.1)},\n",
    "    #{'name': 'flush1', 'type': 'continuous', 'domain': (0.0015*0.9,  0.0045*1.1)},\n",
    "    #{'name': 'equi2', 'type': 'continuous', 'domain': (0.01*0.9, 0.05*1.1)},\n",
    "    {'name': 'dia2', 'type': 'discrete', 'domain': (10*0.9, 50*1.1)},\n",
    "    {'name': 'flush2', 'type': 'continuous', 'domain': (0.00067*0.9,  0.002*1.1)},\n",
    "    #{'name': 'failure', 'type': 'continuous', 'domain': (0.0*0.9, 0.1*1.1)},\n",
    "]\n",
    "\n",
    "# Load inputs and outputs from CSV files\n",
    "input_data = pd.read_csv('inputs.csv')\n",
    "output_data = pd.read_csv('outputs.csv')\n",
    "\n",
    "output_data = output_data[[metric]]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, \n",
    "                                                    output_data, \n",
    "                                                    test_size=0.2, # 20% test, 80% train\n",
    "                                                    random_state=42) # make the random split reproducible\n",
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "Xtrain = X_train.to_numpy(dtype = 'float64')\n",
    "Ytrain = y_train.to_numpy(dtype = 'float64')\n",
    "Xtest = X_test.to_numpy(dtype = 'float64')\n",
    "Ytest = y_test.to_numpy(dtype = 'float64')\n",
    "\n",
    "# Min max normalisation\n",
    "Xmax = np.array([var['domain'][1] for var in space])\n",
    "Xmin = np.array([var['domain'][0] for var in space])\n",
    "\n",
    "XnormP = (Xmax, Xmin)\n",
    "\n",
    "normP = XnormP\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Y_train_standardized = scaler.fit_transform(Ytrain)\n",
    "Y_test_standardized = scaler.transform(Ytest)\n",
    "\n",
    "\n",
    "train_set = (mm_norm(Xtrain, normP),Y_train_standardized)\n",
    "test_set = (mm_norm(Xtest, normP), Y_test_standardized )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuperPro2(p, train_set, normP, space):\n",
    "\n",
    "    ann_loaded = load_model('ann_ACC_0.19_24480_0.0000_437_1.pkl', train_set)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred =ann_loaded(p)\n",
    "        y_pred = scaler.inverse_transform(y_pred)\n",
    "       \n",
    "    return torch.tensor(y_pred)\n",
    "\n",
    "\n",
    "def superpro(parameters, train_set, normP, space):\n",
    "\n",
    "    score = SuperPro2(parameters, train_set, normP, space)\n",
    "\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from gpytorch.kernels import MaternKernel, RBFKernel\n",
    "from botorch.models.transforms import Standardize\n",
    "\n",
    "\n",
    "\n",
    "def initialize_model(X, y, GP=None, state_dict=None, *GP_args, **GP_kwargs):\n",
    "\n",
    "\n",
    "    covar_module = MaternKernel(nu=2.5)\n",
    "\n",
    "\n",
    "    if GP is None:\n",
    "        GP = SingleTaskGP\n",
    "        \n",
    "    model = GP(X, y,  outcome_transform=Standardize(1), covar_module = covar_module, *GP_args, **GP_kwargs).to(X)\n",
    "\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    # load state dict if it is passed\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Repeat from step 1\n",
    "from botorch.optim import optimize_acqf\n",
    "def bo_step(X, y, objective, bounds, GP=None, acquisition=None, q=1, state_dict=None, plot=False):\n",
    "\n",
    "    \n",
    "    mll, gp = initialize_model(X, y, GP=GP, state_dict=state_dict)\n",
    "    fit_gpytorch_model(mll)\n",
    "    \n",
    "    # Create acquisition function\n",
    "    acquisition = acquisition(gp)\n",
    "\n",
    "    # Optimize acquisition function if y is not None\n",
    "\n",
    "    candidate = optimize_acqf(\n",
    "        acquisition, bounds=bounds, q=q,  inequality_constraints=None, num_restarts=50, raw_samples=1024\n",
    "    )\n",
    "       \n",
    "    X = torch.cat([X, candidate[0]])\n",
    "    y = torch.cat([y, objective(candidate[0])])\n",
    "\n",
    "    if plot:\n",
    "        utils.plot_acquisition(acquisition, X, y, candidate)\n",
    "        \n",
    "    return X, y, gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_01 = torch.zeros(2, len(space), dtype=torch.float64)\n",
    "bounds_01[1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_best_params(params, scores, space):\n",
    "    bounds = torch.tensor([var['domain'] for var in space]).to(params).t()\n",
    "    params = unnormalize(params, bounds)\n",
    "    \n",
    "    best_idx = np.argmin(scores.cpu().numpy())\n",
    "    \n",
    "    return wrap_X(params[[best_idx]], space)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition import (ExpectedImprovement, PosteriorMean,\n",
    "                                 ProbabilityOfImprovement,\n",
    "                                 UpperConfidenceBound, qKnowledgeGradient)\n",
    "from botorch.acquisition.max_value_entropy_search import qMaxValueEntropy\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "from botorch.acquisition.max_value_entropy_search import qMaxValueEntropy\n",
    "\n",
    "\n",
    "cum_best_df = pd.DataFrame()\n",
    "best_params_df = pd.DataFrame()\n",
    "\n",
    "execution_times_df = pd.DataFrame()\n",
    "\n",
    "execution_times = []\n",
    "\n",
    "\n",
    "for i in range(0, 100,10) :\n",
    "    print('Seed:', i, 'out of 100')\n",
    "    torch.manual_seed(i)\n",
    "    start_time = time.time()\n",
    "    params = torch.tensor(Xtrain[:5,:])\n",
    "\n",
    "    \n",
    "    scores = torch.tensor(Ytrain[:5])\n",
    "\n",
    "\n",
    "    state_dict = None\n",
    "\n",
    "    budget = 100\n",
    "\n",
    "    objective = lambda x: superpro(x, train_set, normP, space)\n",
    "\n",
    "    # Initialize the counter for consecutive iterations without improvement\n",
    "    no_improvement_count = 0\n",
    "    consecutive_iterations_threshold = 50  # Threshold for consecutive iterations without improvement\n",
    "\n",
    "    best_score = float('+inf')  # Initialize the best score to a very low number\n",
    "\n",
    "    with tqdm.tqdm(total=budget) as bar:\n",
    "        while len(scores) < budget:\n",
    "            n_samples = len(scores)\n",
    "        \n",
    "            # Assuming the rest of your optimization code goes here and updates `scores`\n",
    "            GP = SingleTaskGP\n",
    "        \n",
    "            acquisition = lambda gp: UpperConfidenceBound(gp, beta=15, maximize=False)\n",
    "        \n",
    "\n",
    "            params, scores, gp = bo_step(params, scores, objective, bounds_01, GP=GP, acquisition=acquisition)\n",
    "                                     \n",
    "     \n",
    "            current_best_score = scores.min().item()  # Get the current best score\n",
    "        \n",
    "            # Check if there is an improvement\n",
    "            if current_best_score < best_score:\n",
    "                best_score = current_best_score  # Update the best score\n",
    "                no_improvement_count = 0  # Reset the counter since there was an improvement\n",
    "            else:\n",
    "                no_improvement_count += 1  # Increment the counter\n",
    "        \n",
    "            # Terminate if the number of consecutive iterations without improvement\n",
    "            # reaches the threshold\n",
    "            if no_improvement_count >= consecutive_iterations_threshold:\n",
    "                print(\"Termination criterion met: No improvement for 20 consecutive iterations.\")\n",
    "                break  # Terminate the loop\n",
    "        \n",
    "            bar.update(len(scores) - n_samples)\n",
    "\n",
    "        cum_best = np.minimum.accumulate(scores.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Create a temporary DataFrame from `cum_best` and reindex `cum_best_df` to ensure it's long enough\n",
    "        temp_df = pd.DataFrame({f'{i}': cum_best.squeeze()})\n",
    "        cum_best_df = cum_best_df.reindex(index=range(max(len(cum_best_df), len(temp_df)))).assign(**temp_df)\n",
    "        best_param = get_best_params(params, scores, space)\n",
    "        best_param_values = list(best_param.values())\n",
    "        best_params_df[f'{i}'] = pd.Series(best_param_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================RESULTS==============================================================\n",
    "for key, value in best_param.items():\n",
    "        print(f'{key}: {value}\\n')\n",
    "\n",
    "print('Optimum', scores.min().item())\n",
    "\n",
    "utils.plot_convergence(params, scores, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_best = np.minimum.accumulate(scores.cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2788fb9d5675b44d2f2df23377ba4fb00c0c6d159a9eeaed96789bf0470f2ecc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
