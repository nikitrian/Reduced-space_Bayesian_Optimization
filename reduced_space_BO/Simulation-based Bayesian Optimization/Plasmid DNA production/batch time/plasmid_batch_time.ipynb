{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import utils\n",
    "import tqdm\n",
    "import win32com.client\n",
    "import os\n",
    "\n",
    "device = 'cpu'\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.read_csv('top_inputs.csv')\n",
    "outputs = pd.read_csv('top_outputs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(inputs.values)\n",
    "y = torch.tensor(outputs.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def SuperPro2(p, space):\n",
    "\n",
    "    p = unnormalize_x(p, space)\n",
    "    p = p.reshape(-1,1).numpy()\n",
    "\n",
    "    excelObject = win32com.client.Dispatch(\"Excel.Application\")  #instantiate excel app\n",
    "    excelObject.Visible = True\n",
    "    temp1= os.getcwd()+'\\\\pDNA.xlsm'\n",
    "\n",
    "    \n",
    "    wb = excelObject.Workbooks.Open(temp1)\n",
    "\n",
    "    excelObject.Application.Run('BeforeSuperExcelMatlab')\n",
    "\n",
    "    temp3 =np.shape(p)[1]\n",
    "    F =np.zeros((8,1,temp3))\n",
    "    for i in range(temp3):\n",
    "        \n",
    "        \n",
    "        temp4 = excelObject.Application.Run('SuperExcelMatlab', p[0,i], p[1,i], 0.0033, 0.02045, 0.95, p[2,i], p[3,i], 0.204507, \n",
    "                                            0.98, 150, p[4,i], 0.03, np.round(p[5,i]), p[6,i], 0.02, np.round(40), 0.001333, 0.05)                                      \n",
    "\n",
    "\n",
    "        F[0,0,i] = temp4[0] # Formulation concentration (kg/m3)\n",
    "        F[1,0,i] = temp4[1] #  Productivity (kg/batch)\n",
    "        F[2,0,i] = temp4[2] # CAPEX ($)\n",
    "        F[3,0,i] = temp4[3]/temp4[4] # OPEX ($/batch)\n",
    "        F[4,0,i] = temp4[4] # Number of batches (batches/y)\n",
    "        F[5,0,i] = temp4[5] # Batch time (s)\n",
    "        F[6,0,i] = temp4[6] # Cycle time (s)\n",
    "        F[7,0,i] = temp4[3]/(temp4[4]*(temp4[1]/0.001)) # Cost per gram ($/g)\n",
    "        #F[7,0,i] = temp4[3]/temp4[4] # Opex per batch ($/batch)\n",
    "        \n",
    "\n",
    "    excelObject.Application.Run('AfterSuperExcelMatlab')\n",
    "    \n",
    "    wb.Close(False)\n",
    "    excelObject.Application.Quit()\n",
    "\n",
    "\n",
    "    \n",
    "    return torch.from_numpy(F[5,0,:]).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from gpytorch.kernels import MaternKernel, RBFKernel\n",
    "from botorch.models.transforms import Standardize\n",
    "\n",
    "\n",
    "\n",
    "def initialize_model(X, y, GP=None, state_dict=None, *GP_args, **GP_kwargs):\n",
    "    \"\"\"\n",
    "    Create GP model and fit it. The function also accepts\n",
    "    state_dict which is used as an initialization for the GP model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.tensor, shape=(n_samples, dim)\n",
    "        Input values\n",
    "        \n",
    "    y : torch.tensor, shape=(n_samples,)\n",
    "        Output values\n",
    "        \n",
    "    GP : botorch.models.Model\n",
    "        GP model class\n",
    "        \n",
    "    state_dict : dict\n",
    "        GP model state dict\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mll : gpytorch.mlls.MarginalLoglikelihood\n",
    "        Marginal loglikelihood\n",
    "    \n",
    "    gp : \n",
    "    \"\"\"\n",
    "\n",
    "    covar_module = MaternKernel(nu=2.5)\n",
    "\n",
    "\n",
    "    if GP is None:\n",
    "        GP = SingleTaskGP\n",
    "        \n",
    "    model = GP(X, y,  outcome_transform=Standardize(1), covar_module = covar_module, *GP_args, **GP_kwargs).to(X)\n",
    "\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    # load state dict if it is passed\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Repeat from step 1\n",
    "from botorch.optim import optimize_acqf\n",
    "def bo_step(X, y, objective, bounds, GP=None, acquisition=None, q=1, state_dict=None, plot=False):\n",
    "\n",
    "    \n",
    "    if y is not None:\n",
    "        mll, gp = initialize_model(X, y, GP=GP, state_dict=state_dict)\n",
    "        fit_gpytorch_model(mll)\n",
    "    \n",
    "    # Create acquisition function\n",
    "    acquisition = acquisition(gp)\n",
    "    inequality_constraints = [(torch.tensor([0, 1]), torch.tensor([-1, 1], dtype=torch.float64), torch.tensor(0.0, dtype=torch.float64)),\n",
    "    (torch.tensor([1, 5]), torch.tensor([-1, 1], dtype=torch.float64), torch.tensor(0, dtype=torch.float64))]\n",
    "    # Optimize acquisition function if y is not None\n",
    "    if y is not None:\n",
    "        candidate = optimize_acqf(\n",
    "            acquisition, bounds=bounds, q=q,  inequality_constraints=None, num_restarts=50, raw_samples=1024,\n",
    "        )\n",
    "       \n",
    "        # Check if objective is not None before updating y\n",
    "        y_new = objective(candidate[0].squeeze())\n",
    "        print('y_new=', y_new)\n",
    "        if y_new is not None:\n",
    "            y = torch.cat([y, y_new])\n",
    "            X = torch.cat([X, candidate[0]])\n",
    "        else: \n",
    "            # Repeat until y_random is not None\n",
    "            while True:\n",
    "                X_random = torch.rand(18)\n",
    "                y_rand = objective(X_random)\n",
    "                if y_rand is not None:\n",
    "                    break\n",
    "            X = torch.cat([X, X_random.unsqueeze(0)])\n",
    "            y = torch.cat([y, y_rand])\n",
    "\n",
    "    if plot:\n",
    "        utils.plot_acquisition(acquisition, X, y, candidate)\n",
    "        \n",
    "    return X, y, gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "\n",
    "def normalize_x(params, space):\n",
    "    bounds = torch.tensor([var['domain'] for var in space]).to(params).t()\n",
    "    params = normalize(params, bounds)\n",
    "    return params\n",
    "\n",
    "def unnormalize_x(params, space):\n",
    "    bounds = torch.tensor([var['domain'] for var in space]).to(params).t()\n",
    "    params = unnormalize(params, bounds)\n",
    "    return params\n",
    "\n",
    "def wrap_X(X, space):\n",
    "    \"\"\"\n",
    "    Wrap tensor to a list of dictionaries\n",
    "    \n",
    "    Parameters:\n",
    "    X : torch.tensor, shape=(n_samples, dim)\n",
    "        Tensor of parameters values\n",
    "        \n",
    "    space : dict\n",
    "        Search space description\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    wrapped_X : lsit[dict{param_name: value}]\n",
    "        List of dictionary of parameters\n",
    "    \"\"\"\n",
    "    def _wrap_row(row):\n",
    "        wrapped_row = {}\n",
    "        for i, x in enumerate(row):\n",
    "            wrapped_row[space[i]['name']] = x.item()\n",
    "        \n",
    "            if space[i]['type'] == 'discrete':\n",
    "                wrapped_row[space[i]['name']] = int(np.round(x.item()))\n",
    "        return wrapped_row\n",
    "    \n",
    "    wrapped_X = []\n",
    "    for i in range(X.shape[0]):\n",
    "        wrapped_X.append(_wrap_row(X[i]))\n",
    "        \n",
    "    return wrapped_X\n",
    "\n",
    "\n",
    "def unwrap_X(parameters, space):\n",
    "    \"\"\"\n",
    "    Unwrap list of dictionaries to torch.tensor\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters : list(dict)\n",
    "        List of parameters\n",
    "        \n",
    "    space : dict\n",
    "        Input space definition\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    unwrapped_X : torch.tensor\n",
    "        Tensor of parameter values\n",
    "    \"\"\"\n",
    "    X = torch.zeros(len(parameters), len(space),\n",
    "                    dtype=torch.float64)\n",
    "    for i, p in enumerate(parameters):\n",
    "        x = [p[var['name']] for var in space]\n",
    "        X[i] = torch.tensor(x, dtype=torch.float64)\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def superpro(parameters,space):\n",
    "\n",
    "\n",
    "    #Objective function\n",
    "\n",
    "    score = SuperPro2(parameters,space)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "# Define the region\n",
    "\n",
    "space = [\n",
    "    {'name': 'flask_time', 'type': 'continuous', 'domain': (21600, 108000)},\n",
    "    {'name': 'seed_time', 'type': 'continuous', 'domain': (43200, 108000)},\n",
    "    #{'name': 'fed_batch_seed', 'type': 'continuous', 'domain': (0.001636, 0.004908)},\n",
    "    #{'name': 'batch_seed', 'type': 'continuous', 'domain': (0.010225325, 0.030675975)},\n",
    "    #{'name': 'conversion_seed', 'type': 'continuous', 'domain': (0.9, 0.98)},\n",
    "    {'name': 'main_time', 'type': 'continuous', 'domain': (64800, 144000)},\n",
    "    {'name': 'fed_batch', 'type': 'continuous', 'domain': (0.0190875, 0.0572625)},\n",
    "    #{'name': 'batch_med', 'type': 'continuous', 'domain': (0.102253255, 0.306759765)},\n",
    "    #{'name': 'conversion_main', 'type': 'continuous', 'domain': (0.9, 0.98)},\n",
    "    #{'name': 'solid_conc', 'type': 'continuous', 'domain': (50, 300)},\n",
    "    {'name': 'res_vol', 'type': 'continuous', 'domain': (0.28903585,  0.86710755)},\n",
    "    #{'name': 'equi1', 'type': 'continuous', 'domain': (0.01, 0.05)},\n",
    "    {'name': 'dia1', 'type': 'discrete', 'domain': (10, 50)},\n",
    "    {'name': 'flush1', 'type': 'continuous', 'domain': (0.0015,  0.0045)},\n",
    "    #{'name': 'equi2', 'type': 'continuous', 'domain': (0.01, 0.05)},\n",
    "    #{'name': 'dia2', 'type': 'discrete', 'domain': (10, 50)},\n",
    "    #{'name': 'flush2', 'type': 'continuous', 'domain': (0.00067,  0.002)},\n",
    "    #{'name': 'failure', 'type': 'continuous', 'domain': (0,  0.1)},\n",
    "]\n",
    "\n",
    "\n",
    "bounds_01 = torch.zeros(2, len(space), dtype=torch.float64)\n",
    "bounds_01[1] = 1\n",
    "\n",
    "init_X = draw_sobol_samples(bounds_01, 6, 1).squeeze()\n",
    "\n",
    "init_y = []\n",
    "valid_init_X = []\n",
    "\n",
    "while len(init_y) < 5:\n",
    "    for i in range(len(init_X)):\n",
    "        result = superpro(init_X[i], space)\n",
    "        if result is not None:\n",
    "            init_y.append(result)\n",
    "            valid_init_X.append(init_X[i])\n",
    "    \n",
    "    # If len(init_y) is still less than 5, generate additional Sobol samples\n",
    "    if len(init_y) < 5:\n",
    "        additional_samples = draw_sobol_samples(bounds_01, 5 - len(init_y), 1).squeeze()\n",
    "        init_X = torch.cat([init_X, additional_samples])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_y2= torch.tensor(init_y)\n",
    "init_y2= init_y2.reshape(-1,1)\n",
    "init_X2 = torch.stack(valid_init_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "init_X2 = torch.cat([normalize_x(X, space), init_X2])\n",
    "\n",
    "init_y2 = torch.cat([y, init_y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_best_params(params, scores, space):\n",
    "    bounds = torch.tensor([var['domain'] for var in space]).to(params).t()\n",
    "    params = unnormalize(params, bounds)\n",
    "    \n",
    "    best_idx = np.argmin(scores.cpu().numpy())\n",
    "    \n",
    "    return wrap_X(params[[best_idx]], space)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition import (ExpectedImprovement, PosteriorMean,\n",
    "                                 ProbabilityOfImprovement,\n",
    "                                 UpperConfidenceBound, qKnowledgeGradient)\n",
    "from botorch.acquisition.max_value_entropy_search import qMaxValueEntropy\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "from botorch.acquisition.max_value_entropy_search import qMaxValueEntropy\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cum_best_df = pd.DataFrame()\n",
    "best_params_df = pd.DataFrame()\n",
    "\n",
    "for i in range(0, 10,10) :\n",
    "    print('Seed:', i, 'out of 100')\n",
    "    torch.manual_seed(i)\n",
    "\n",
    "\n",
    "\n",
    "    params = init_X2\n",
    "\n",
    "    scores = init_y2\n",
    "\n",
    "    state_dict = None\n",
    "\n",
    "    budget = 116\n",
    "\n",
    "    objective = lambda x: superpro(x, space)\n",
    "\n",
    "    # Initialize the counter for consecutive iterations without improvement\n",
    "    no_improvement_count = 0\n",
    "    consecutive_iterations_threshold = 20  # Threshold for consecutive iterations without improvement\n",
    "\n",
    "    best_score = float('+inf')  # Initialize the best score to a very low number\n",
    "\n",
    "    with tqdm.tqdm(total=budget) as bar:\n",
    "        while len(scores) < budget:\n",
    "            n_samples = len(scores)\n",
    "        \n",
    "            # Assuming the rest of your optimization code goes here and updates `scores`\n",
    "            GP = SingleTaskGP\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "            acquisition = lambda gp: UpperConfidenceBound(gp, beta=15, maximize=False)\n",
    "        \n",
    "\n",
    "            params, scores, gp = bo_step(params, scores, objective, bounds_01,\n",
    "                                     GP=GP, acquisition=acquisition\n",
    "                                     )\n",
    "\n",
    "            current_best_score = scores.min().item()  # Get the current best score\n",
    "        \n",
    "            # Check if there is an improvement\n",
    "            if current_best_score < best_score:\n",
    "                best_score = current_best_score  # Update the best score\n",
    "                no_improvement_count = 0  # Reset the counter since there was an improvement\n",
    "            else:\n",
    "                no_improvement_count += 1  # Increment the counter\n",
    "        \n",
    "            # Terminate if the number of consecutive iterations without improvement\n",
    "            # reaches the threshold\n",
    "            if no_improvement_count >= consecutive_iterations_threshold:\n",
    "                print(\"Termination criterion met: No improvement for 20 consecutive iterations.\")\n",
    "                break  # Terminate the loop\n",
    "        \n",
    "            bar.update(len(scores) - n_samples)\n",
    "\n",
    "\n",
    "        import sys\n",
    "\n",
    "        cum_best = np.minimum.accumulate(scores.cpu().numpy())\n",
    "\n",
    "        # Create a temporary DataFrame from `cum_best` and reindex `cum_best_df` to ensure it's long enough\n",
    "        temp_df = pd.DataFrame({f'{i}': cum_best.squeeze()})\n",
    "        cum_best_df = cum_best_df.reindex(index=range(max(len(cum_best_df), len(temp_df)))).assign(**temp_df)\n",
    "        best_param = get_best_params(params, scores, space)\n",
    "        best_param_values = list(best_param.values())\n",
    "        best_params_df[f'{i}'] = pd.Series(best_param_values)\n",
    "\n",
    "        np.set_printoptions(threshold=sys.maxsize)\n",
    "        a= cum_best.squeeze()\n",
    "        for j in range(len(a)):\n",
    "            print(a[j])\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================RESULTS==============================================================\n",
    "for key, value in best_param.items():\n",
    "        print(f'{key}: {value}\\n')\n",
    "\n",
    "print('Optimum', scores.min().item())\n",
    "\n",
    "utils.plot_convergence(params, scores, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cum_best = np.minimum.accumulate(scores.cpu().numpy())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2788fb9d5675b44d2f2df23377ba4fb00c0c6d159a9eeaed96789bf0470f2ecc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
